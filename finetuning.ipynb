{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•„ë˜ì˜ ê¹ƒí—ˆë¸Œë¥¼ ì°¸ê³ í•˜ì—¬ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. \n",
    "\n",
    "* https://github.com/facebookresearch/llama\n",
    "* https://github.com/facebookresearch/llama-recipes\n",
    "* https://github.com/facebookresearch/llama-recipes/blob/main/quickstart.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download.shë¡œ ë‹¤ìš´ ë°›ì€ ë¼ë§ˆ ê°€ì¤‘ì¹˜ë¥¼ í—ˆê¹…í˜ì´ìŠ¤(hf)ë¡œ ë°”ê¿”ì•¼ ë¡œë“œê°€ëŠ¥ <br>\n",
    "ë°”ê¾¸ëŠ” ë°©ë²•ì€ ì•„ë˜ ì½”ë“œ ì‹¤í–‰í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ê°€ìƒí™˜ê²½ì—ì„œ ì•„ë˜ì˜ ê²½ë¡œì— modelsë€ í´ë”ë¥¼ ë§Œë“¤ê³  download.shë¡œ ë‹¤ìš´ë°›ì€ ê²ƒì„ models í´ë”ì— ë„£ëŠ”ë‹¤. <br>\n",
    "../site-packages/transformers/models/llama\n",
    "* ì¦‰, ../site-packages/transformers/models/llamaë¥¼ ë“¤ì–´ê°€ë©´ models/7B/\"ë‚´ê°€ ë‹¤ìš´ ë°›ì€ params.json. ...pth, ...chk\" ê°€ ìˆê³   \n",
    "modelsì™€ ê°™ì€ ê²½ë¡œì— tokenizer.modelì´ ìˆì–´ì•¼ í•œë‹¤.\n",
    "* ë‹¤ ë˜ì—ˆìœ¼ë©´ ì•„ë˜ì˜ ì£¼ì„ ì½”ë“œë¥¼ bash ì‰˜ë¡œ ì‹¤í–‰í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets\n",
    "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weights_to_hf.py')\"`\n",
    "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Point model_id to model weight folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/models_hf/7B-chat'\n",
    "PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA ì„¤ì¹˜ ì•ˆë˜ì–´ìˆìœ¼ë©´ ì˜¤ë¥˜ë‚©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=PATH\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the base model only repeats the conversation.\n",
    "\n",
    "### Step 4: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT : íš¨ìœ¨ì ì¸ íŒŒë¼ë¯¸í„° íŠœë‹, í•™ìŠµ ì‹œê°„ì€ ì¤„ì´ê³  ì„±ëŠ¥ì€ full parameter finetuningê³¼ ë¹„ìŠ·í•¨.\n",
    "* https://github.com/facebookresearch/llama-recipes/blob/main/docs/LLM_finetuning.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë“¤ì€ ëª¨ë‘ llama_recipesì˜ ê¹ƒí—ˆë¸Œë¥¼ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define an optional profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enable_profiler = False\n",
    "output_dir = \"tmp/llama-output\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv_path ë³€ìˆ˜ì— csvíŒŒì¼ ê²½ë¡œë¥¼ í• ë‹¹í•œë‹¤. ì´ë•Œ í•™ìŠµì‹œí‚¬ ì»¬ëŸ¼ëª…ì€ textë¡œ ì„¤ì •í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë˜í•œ ì§ˆì˜ë¬¸ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì˜ ì½”ë“œë¥¼ ì¶”ê°€í•œ í›„ SFTTrainerì˜ ì¸ìì—ì„œ formatting_funcì˜ ì£¼ì„ì„ í•´ì œí•œë‹¤.\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"[INST] <<SYS>>\\n{example['question']}\\n<</SYS>>\\n\\n{example['answer']} [/INST]\"        # ì‹œì‘ í† í°ì´ <s>ë¡œ ìë™ìœ¼ë¡œ ë¶™ê¸° ë•Œë¬¸ì— ë§¨ ì• <s> í† í°ì€ ìƒëµí•œë‹¤.\n",
    "    return text\n",
    "```\n",
    "\n",
    "* ì´ë•Œ csv íŒŒì¼ì—ì„œ ì§ˆë¬¸ ì»¬ëŸ¼ëª…ì€ \"question\", ë‹µë³€ ì»¬ëŸ¼ëª…ì€ \"answer\"ì´ì–´ì•¼ í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt template llama-2\n",
    "\n",
    "- llama-2 ëª¨ë¸ì—ì„œì˜ í”„ë¡¬í”„íŠ¸ í¬ë§·ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "- ì¶œì²˜ : https://huggingface.co/blog/llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "prompt template llama-2\n",
    "\n",
    "\n",
    "<s>[INST] <<SYS>>\n",
    "{{ system_prompt }}\n",
    "<</SYS>>\n",
    "\n",
    "{{ user_message }} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "example\n",
    "\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "There's a llama in my garden ğŸ˜± What should I do? [/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ì—°ì†ëœ ì±—ë´‡ê³¼ ìœ ì €ì˜ ì§ˆì˜ì‘ë‹µì˜ ê²½ìš°\n",
    "\n",
    "\n",
    "<s>[INST] <<SYS>>\n",
    "{{ system_prompt }}\n",
    "<</SYS>>\n",
    "\n",
    "{{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST]\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '...'\n",
    "df = pd.read_csv(csv_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒì˜ ë‘ ê°€ì§€ë¥¼ ë°”ê¿”ê°€ë©° GPU RAM ìš©ëŸ‰ì„ ì²´í¬í•´ë³´ì„¸ìš”. <br>\n",
    "per_device_train_batch_size, max_seq_length \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,      # ë®ì–´ì“°ê¸°\n",
    "    bf16=True,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,       # 10 stepë§ˆë‹¤ ê¸°ë¡\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        data_collator=default_data_collator,\n",
    "        max_seq_length = 1500,          # ê¸¸ì–´ì§€ë©´ GPU ì†Œëª¨ëŸ‰ ìƒìŠ¹\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "        packing=True,    # chat gpt : ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ë ¤ë©´ packing=Trueë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "        tokenizer = tokenizer,\n",
    "        # formatting_func=formatting_func       # ì§ˆì˜ë¬¸ìœ¼ë¡œ í•™ìŠµì‹œì— ì£¼ì„ í•´ì œí•˜ê¸°\n",
    "    )\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:\n",
    "Save model checkpoint <br>\n",
    "saved only lora weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)   # output_dir : tmp/llama-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8:\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"Question on the your csv file.\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9:\n",
    "\n",
    "load your saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model_id, you can use the one downloaded from download.sh or model_id from the hugging face. <br>\n",
    "\n",
    "model_id ëŠ” download.sh ì—ì„œ ë‹¤ìš´ ë°›ì€ ê²ƒì´ë‚˜ í—ˆê¹… í˜ì´ìŠ¤ì˜ model_idë¥¼ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_peft_model(model, peft_model):\n",
    "    peft_model = PeftModel.from_pretrained(model, peft_model)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    return merged_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.float16)     # load pretrained model.\n",
    "peft_model = 'tmp/llama-output/'\n",
    "merged_model = load_peft_model(model, peft_model)      # merged lora weight and pretrained model.\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"Question on the your csv file.\"\n",
    "max_new_tokens = 100            # ìƒì„± ë¬¸ì¥ ê¸¸ì´\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "merged_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(merged_model.generate(**model_input, max_new_tokens=max_new_tokens)[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
